import streamlit as st
import os
import tempfile
import psycopg2
from dotenv import load_dotenv
from psycopg2.extras import execute_values
from datetime import datetime
import json
from typing import List, Optional
import requests

# –ò–º–ø–æ—Ä—Ç—ã LlamaIndex
from llama_index.core import (
    VectorStoreIndex,
    Document,
    StorageContext,
    Settings,
    load_index_from_storage,
)
from llama_index.llms.openrouter import OpenRouter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.core.ingestion import IngestionPipeline

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
import PyPDF2
from docx import Document as DocxDocument
import pandas as pd
import hashlib

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="—Å–ø–∏–Ω—Ç–µ—Ö—Ç—ë–Ω–æ–∫",
    page_icon="üéì",
    layout="wide"
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class Config:
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ PostgreSQL
    DB_NAME = os.environ.get("DB_NAME")
    DB_USER = os.environ.get("DB_LOGIN")
    DB_PASSWORD = os.environ.get("DB_PASSWORD")
    DB_HOST = "localhost"
    DB_PORT = int("5432")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OpenRouter
    OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
    OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
    LLM_MODEL = os.environ.get("OPENROUTER_MODEL")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL")
    EMBEDDING_DIM = int(os.environ.get("EMBEDDING_DIM"))
    
    # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50

def init_db():
    try:
        conn = psycopg2.connect(
            dbname=Config.DB_NAME,
            user=Config.DB_USER,
            password=Config.DB_PASSWORD,
            host=Config.DB_HOST,
            port=Config.DB_PORT
        )
        return conn
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def create_tables():
    conn = init_db()
    if not conn:
        return
    
    cur = conn.cursor()
    
    cur.execute("""
        CREATE TABLE IF NOT EXISTS documents (
            id SERIAL PRIMARY KEY,
            filename VARCHAR(255) NOT NULL,
            file_type VARCHAR(50) NOT NULL,
            file_size BIGINT,
            file_hash VARCHAR(64) UNIQUE,
            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            content TEXT,
            metadata JSONB,
            processed BOOLEAN DEFAULT FALSE
        )
    """)
    
    conn.commit()
    cur.close()
    conn.close()

    # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —á–∞–Ω–∫–æ–≤ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ LlamaIndex

@st.cache_resource
def init_models():
    llm = OpenRouter(
        model=Config.LLM_MODEL,
        api_key=Config.OPENROUTER_API_KEY,
        base_url=Config.OPENROUTER_BASE_URL,
        temperature=0.1,
        context_window=4096
    )
    
    embed_model = HuggingFaceEmbedding(
        model_name=Config.EMBEDDING_MODEL
    )
    
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    return llm, embed_model

@st.cache_resource
def init_vector_store():
    try:
        vector_store = PGVectorStore.from_params(
            database=Config.DB_NAME,
            host=Config.DB_HOST,
            password=Config.DB_PASSWORD,
            port=Config.DB_PORT,
            user=Config.DB_USER,
            table_name="document_chunks",
            embed_dim=Config.EMBEDDING_DIM
        )
        return vector_store
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
        return None

def extract_text_from_file(file_path, file_type):
    text = ""
    
    try:
        if file_type == "pdf":
            with open(file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        
        elif file_type == "docx":
            doc = DocxDocument(file_path)
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
        
        elif file_type == "txt":
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()
        
        return text.strip()
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return ""

from bs4 import BeautifulSoup

def extract_text_from_url(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
        response.encoding = response.apparent_encoding
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 
                            'iframe', 'noscript', 'svg', 'form', 'button', 'input']):
            element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        text_parts = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Ç–µ–≥–∏
        content_tags = ['article', 'main', 'section.content', 'div.content', 
                       'div.article', 'div.post', 'div.entry-content']
        
        for tag in content_tags:
            if ',' in tag:
                selector, class_name = tag.split('.')
                elements = soup.find_all(selector, class_=class_name)
            else:
                elements = soup.find_all(tag)
            
            if elements:
                for element in elements:
                    text = element.get_text(strip=True, separator='\n')
                    if len(text) > 100:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞
                        text_parts.append(text)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –±–ª–æ–∫–∏, –±–µ—Ä–µ–º –≤–µ—Å—å body
        if not text_parts:
            body = soup.find('body')
            if body:
                text = body.get_text(strip=True, separator='\n')
                text_parts.append(text)
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –±–µ—Ä–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç
        if not text_parts:
            text = soup.get_text(strip=True, separator='\n')
            text_parts.append(text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç
        full_text = '\n\n'.join(text_parts)
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        import re
        full_text = re.sub(r'\n{3,}', '\n\n', full_text)  # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
        full_text = re.sub(r'\s{2,}', ' ', full_text)     # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if len(full_text) > 100000:
            full_text = full_text[:100000] + "\n\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞]"
        
        return full_text.strip()
    
    except requests.exceptions.RequestException as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return ""
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return ""

def clean_text(text):
    """
    –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    
    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    
    # –£–±–∏—Ä–∞–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏
    text = re.sub(r'&[a-z]+;', ' ', text)
    
    # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        if len(line) > 1000:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            sentences = re.split(r'(?<=[.!?])\s+', line)
            cleaned_lines.extend(sentences)
        else:
            cleaned_lines.append(line)
    
    text = '\n'.join(cleaned_lines)
    
    return text.strip()

def is_valid_url(url):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
    """
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def calculate_file_hash(file_path):
    hasher = hashlib.sha256()
    with open(file_path, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def upload_document(file, file_type, url=None):
    conn = init_db()
    if not conn:
        return False
    
    tmp_file_path = None
    try:
        if url:
            # –î–ª—è URL —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º
            content = extract_text_from_url(url)
            if not content:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã")
                return False
            
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as tmp_file:
                tmp_file.write(content)
                tmp_file_path = tmp_file.name
                file_hash = hashlib.sha256(url.encode()).hexdigest()
        else:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp_file:
                tmp_file.write(file.read())
                tmp_file_path = tmp_file.name
                file_hash = calculate_file_hash(tmp_file_path)
                content = extract_text_from_file(tmp_file_path, file_type)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        cur = conn.cursor()
        cur.execute("SELECT id FROM documents WHERE file_hash = %s", (file_hash,))
        if cur.fetchone():
            st.warning("–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Ä–∞–Ω–µ–µ!")
            return False
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        metadata = {
            "source": url if url else file.name,
            "upload_date": datetime.now().isoformat(),
            "original_url": url if url else None
        }
        
        cur.execute("""
            INSERT INTO documents 
            (filename, file_type, file_size, file_hash, content, metadata)
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (
            url if url else file.name,
            "url" if url else file_type,
            len(content.encode('utf-8')) if content else 0,
            file_hash,
            content,
            json.dumps(metadata)
        ))
        
        doc_id = cur.fetchone()[0]
        conn.commit()
        
        if content:
            process_document(content, doc_id, metadata)
            cur.execute("UPDATE documents SET processed = TRUE WHERE id = %s", (doc_id,))
            conn.commit()
        
        st.success("–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
        return True
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        conn.rollback()
        return False
    
    finally:
        conn.close()
        if tmp_file_path and os.path.exists(tmp_file_path):
            os.unlink(tmp_file_path)

def process_document(content, doc_id, metadata):
    try:
        doc = Document(
            text=content,
            metadata={
                "doc_id": doc_id,
                **metadata
            }
        )
        
        splitter = SemanticSplitterNodeParser(
            buffer_size=1,
            breakpoint_percentile_threshold=95,
            embed_model=Settings.embed_model
        )
        
        pipeline = IngestionPipeline(
            transformations=[
                splitter,
                Settings.embed_model
            ],
            vector_store=init_vector_store()
        )
        
        nodes = pipeline.run(documents=[doc])
        
        return True
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        return False

def get_documents():
    conn = init_db()
    if not conn:
        return []
    
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT id, filename, file_type, file_size, upload_date, processed
            FROM documents 
            ORDER BY upload_date DESC
        """)
        
        documents = cur.fetchall()
        return documents
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        return []
    
    finally:
        conn.close()

def delete_document(doc_id):
    conn = init_db()
    if not conn:
        return False
    
    try:
        cur = conn.cursor()
        
        vector_store = init_vector_store()
        if vector_store:
            # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —É–¥–∞–ª–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –ø–æ doc_id
            pass
        
        cur.execute("DELETE FROM documents WHERE id = %s", (doc_id,))
        conn.commit()
        
        return True
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        conn.rollback()
        return False
    
    finally:
        conn.close()

@st.cache_resource
def init_rag_system():
    try:
        vector_store = init_vector_store()
        if not vector_store:
            return None
        
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        try:
            index = load_index_from_storage(storage_context)
        except:
            index = VectorStoreIndex.from_vector_store(
                vector_store=vector_store,
                embed_model=Settings.embed_model
            )
        
        query_engine = index.as_query_engine(
            llm=Settings.llm,
            similarity_top_k=3,
            system_prompt="""–¢—ã - '–°–ü–ò–ù–¢–µ—Ö—Ç—ë–Ω–æ–∫', —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ –°–ü–ò–ù–¢–µ—Ö –≤ –ù–ò–£ –ú–ò–≠–¢.
            –¢—ã –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—á–∞—Ç—å –¢–û–õ–¨–ö–û –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –ò–Ω—Å—Ç–∏—Ç—É—Ç–∞ –°–ü–ò–ù–¢–µ—Ö –∏ –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏ '–î–µ–Ω—å –°–ü–ò–ù–¢–µ—Ö–∞'.
            –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–≤—è–∑–∞–Ω —Å —ç—Ç–∏–º–∏ —Ç–µ–º–∞–º–∏, –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å –æ—Ç–≤–µ—á–∞—Ç—å, –æ–±—ä—è—Å–Ω–∏–≤ —Å–≤–æ—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é.
            –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–æ—á–Ω—ã—Ö –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤."""
        )
        
        return query_engine
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã: {e}")
        return None

def classify_intent(user_input: str, llm: OpenRouter) -> str:
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ. –í—ã–±–µ—Ä–∏ –¢–û–õ–¨–ö–û –û–î–ù–£ –∏–∑ —Ç—Ä–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:

1. chitchat - –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
   - –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–æ—â–∞–µ—Ç—Å—è
   - –ó–∞–¥–∞–µ—Ç –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Ç–µ–±–µ –∏–ª–∏ —Ç–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö
   - –ü—Ä–æ—Å—Ç–æ –±–µ—Å–µ–¥—É–µ—Ç –Ω–∞ –æ—Ç–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–º—ã
   - –ü—Ä–∏–º–µ—Ä—ã: "–ü—Ä–∏–≤–µ—Ç", "–ö–∞–∫ –¥–µ–ª–∞?", "–ß–µ–º —Ç—ã –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è?", "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ"

2. knowledge - –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
   - –ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ –°–ü–ò–ù–¢–µ—Ö
   - –°–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏ "–î–µ–Ω—å –°–ü–ò–ù–¢–µ—Ö–∞"
   - –ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –°–ü–ò–ù–¢–µ—Ö–µ
   - –ü—Ä–∏–º–µ—Ä—ã: "–ö–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–∞–Ω –°–ü–ò–ù–¢–µ—Ö?", "–ß—Ç–æ –±—É–¥–µ—Ç –Ω–∞ –î–Ω–µ –°–ü–ò–ù–¢–µ—Ö–∞?", "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∞—Ö"

3. out_of_scope - –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:
   - –ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –Ω–µ –ø–æ —Ç–µ–º–µ –°–ü–ò–ù–¢–µ—Ö–∞
   - –ü—Ä–æ—Å–∏—Ç —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ-—Ç–æ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–µ –∏–ª–∏ –Ω–µ—ç—Ç–∏—á–Ω–æ–µ
   - –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –±–µ—Å—Å–º—ã—Å–ª–∏—Ü—É
   - –ü—Ä–∏–º–µ—Ä—ã: "–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–æ—Ä—Ç?", "–í–∑–ª–æ–º–∞–π —Å–∞–π—Ç", "asdfghjkl"

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_input}"

–¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¢–û–õ–¨–ö–û –æ–¥–Ω–æ —Å–ª–æ–≤–æ: chitchat, knowledge –∏–ª–∏ out_of_scope.
–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π, —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—é."""

    try:
        response = llm.complete(prompt, max_tokens=10)
        intent = response.text.strip().lower()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        valid_intents = ['chitchat', 'knowledge', 'out_of_scope']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –æ–¥–Ω—É –∏–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for valid_intent in valid_intents:
            if valid_intent in intent:
                return valid_intent
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ª–æ–≥–∏–∫—É
        return fallback_intent_classification(user_input)
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è: {e}")
        raise

def get_intent_specific_response(intent: str, user_input: str, query_engine=None):
    if intent == 'knowledge':
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º RAG –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –°–ü–ò–ù–¢–µ—Ö—É
        if query_engine:
            response = query_engine.query(user_input)
            return response.response
        else:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
    
    elif intent == 'chitchat':
        # –î–ª—è –±–µ—Å–µ–¥—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        return "–Ø - –°–ü–ò–ù–¢–µ—Ö—Ç—ë–Ω–æ–∫, —Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ –°–ü–ò–ù–¢–µ—Ö. –ü–æ–º–æ–≥–∞—é —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ–± –∏—Å—Ç–æ—Ä–∏–∏ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞ –∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏ '–î–µ–Ω—å –°–ü–ò–ù–¢–µ—Ö–∞'. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
    
    elif intent == 'out_of_scope':
        # –í–µ–∂–ª–∏–≤—ã–π –æ—Ç–∫–∞–∑ –¥–ª—è –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Å—Ç–∏—Ç—É—Ç–æ–º –°–ü–ò–ù–¢–µ—Ö –∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ–º '–î–µ–Ω—å –°–ü–ò–ù–¢–µ—Ö–∞'. –ú–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ."

def chat_interface():
    st.title("—Å–ø–∏–Ω—Ç–µ—Ö—Ç—ë–Ω–æ–∫")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "query_engine" not in st.session_state:
        with st.spinner("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞..."):
            st.session_state.query_engine = init_rag_system()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if "intent_llm" not in st.session_state:
        st.session_state.intent_llm = OpenRouter(
            model=Config.LLM_MODEL,
            api_key=Config.OPENROUTER_API_KEY,
            base_url=Config.OPENROUTER_BASE_URL,
            temperature=0.1,
            context_window=4096
        )
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ –°–ü–ò–ù–¢–µ—Ö–µ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("–°–ü–ò–ù–¢–µ—Ö—Ç—ë–Ω–æ–∫ –¥—É–º–∞–µ—Ç..."):
                try:
                    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
                    intent = classify_intent(prompt, st.session_state.intent_llm)
                    
                    # 2. –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ—Ç–≤–µ—Ç
                    answer = get_intent_specific_response(
                        intent, 
                        prompt, 
                        st.session_state.query_engine
                    )
                    
                except Exception as e:
                    answer = f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"
                
                st.markdown(answer)
        
        st.session_state.messages.append({"role": "assistant", "content": answer})

def admin_interface():
    st.title("‚öôÔ∏è –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–Ω–µ–ª—å")
    
    tab1, tab2, tab3 = st.tabs(["üì§ –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã", "üìã –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏"])
    
    with tab1:
        st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤")
            file_type = st.selectbox(
                "–¢–∏–ø —Ñ–∞–π–ª–∞",
                ["pdf", "docx", "txt"]
            )
            
            uploaded_file = st.file_uploader(
                f"–í—ã–±–µ—Ä–∏—Ç–µ {file_type.upper()} —Ñ–∞–π–ª",
                type=[file_type]
            )
            
            if uploaded_file and st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"):
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞..."):
                    success = upload_document(uploaded_file, file_type)
                    if success:
                        st.rerun()
        
        with col2:
            st.markdown("#### –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            url = st.text_input("URL –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            
            if url and st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É"):
                with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã..."):
                    success = upload_document(None, "html", url)
                    if success:
                        st.rerun()
        
        st.markdown("---")
        st.info("""
        **–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:**
        - PDF, DOCX, TXT —Ñ–∞–π–ª—ã
        - –í–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã (—á–µ—Ä–µ–∑ URL)
        
        **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏:**
        1. –î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        2. –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        3. –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
        4. –ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        """)
    
    with tab2:
        st.subheader("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        
        documents = get_documents()
        
        if not documents:
            st.info("–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            df = pd.DataFrame(documents, columns=["ID", "–ò–º—è —Ñ–∞–π–ª–∞", "–¢–∏–ø", "–†–∞–∑–º–µ—Ä", "–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏", "–û–±—Ä–∞–±–æ—Ç–∞–Ω"])
            df["–†–∞–∑–º–µ—Ä"] = df["–†–∞–∑–º–µ—Ä"].apply(lambda x: f"{x} –±–∞–π—Ç" if x else "N/A")
            df["–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏"] = pd.to_datetime(df["–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏"]).dt.strftime("%Y-%m-%d %H:%M")
            df["–°—Ç–∞—Ç—É—Å"] = df["–û–±—Ä–∞–±–æ—Ç–∞–Ω"].apply(lambda x: "‚úÖ" if x else "‚è≥")
            
            st.dataframe(df.drop(columns=["–û–±—Ä–∞–±–æ—Ç–∞–Ω"]), width=True)
            
            st.markdown("### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
            doc_id_to_delete = st.number_input("ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è", min_value=1, step=1)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç", type="secondary"):
                    if doc_id_to_delete:
                        if delete_document(doc_id_to_delete):
                            st.success("–î–æ–∫—É–º–µ–Ω—Ç —É–¥–∞–ª–µ–Ω!")
                            st.rerun()
                    else:
                        st.warning("–í–≤–µ–¥–∏—Ç–µ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            
            with col2:
                if st.button("–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫", type="secondary"):
                    st.rerun()
    
    with tab3:
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã")
        
        st.markdown("### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π")
        st.info(f"**LLM –º–æ–¥–µ–ª—å:** {Config.LLM_MODEL}")
        st.info(f"**Embedding –º–æ–¥–µ–ª—å:** {Config.EMBEDDING_MODEL}")
        
        st.markdown("### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        conn = init_db()
        if conn:
            cur = conn.cursor()
            
            cur.execute("SELECT COUNT(*) FROM documents")
            doc_count = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM documents WHERE processed = TRUE")
            processed_count = cur.fetchone()[0]
            
            try:
                cur.execute("SELECT COUNT(*) FROM document_chunks")
                chunk_count = cur.fetchone()[0] or 0
            except:
                chunk_count = 0
            
            conn.close()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", doc_count)
            with col2:
                st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", processed_count)
            with col3:
                st.metric("–ß–∞–Ω–∫–æ–≤ –≤ –±–∞–∑–µ", chunk_count)
        
        st.markdown("### –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞", type="secondary"):
            st.session_state.messages = []
            st.success("–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –æ—á–∏—â–µ–Ω–∞!")
        
        if st.button("–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã", type="secondary"):
            st.warning("–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ")

def main():
    create_tables()
    
    init_models()
    
    with st.sidebar:
        page = st.radio(
            "–ù–∞–≤–∏–≥–∞—Ü–∏—è",
            ["üí¨ –ß–∞—Ç–±–æ—Ç", "‚öôÔ∏è –ê–¥–º–∏–Ω–∫–∞"]
        )
    
    if page == "üí¨ –ß–∞—Ç–±–æ—Ç":
        chat_interface()
    else:
        admin_interface()

if __name__ == "__main__":
    main()
